{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4748b8e",
   "metadata": {},
   "source": [
    "Async OpenAI client wrapper with retry & singleton semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import openai  # type: ignore\n",
    "\n",
    "from src.core.config import config\n",
    "from src.core.logger import log\n",
    "\n",
    "__all__ = [\"OpenAIClient\", \"get_openai_client\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6437c",
   "metadata": {},
   "source": [
    "# Constant settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134623",
   "metadata": {},
   "outputs": [],
   "source": [
    "_MAX_RETRIES = 4\n",
    "_BASE_BACKOFF = 1.0  # seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273826e4",
   "metadata": {},
   "source": [
    "# Vision-capable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624749f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISION_MODELS = [\n",
    "    \"gpt-4-vision-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b0302",
   "metadata": {},
   "source": [
    "# Fallback model for non-vision requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33695",
   "metadata": {},
   "source": [
    "Lightweight async wrapper around OpenAI chat completion API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735347fe",
   "metadata": {},
   "source": [
    "Return the singleton instance, creating it on first use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec0dae",
   "metadata": {},
   "source": [
    "Initialize the underlying async OpenAI client (internal use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9685b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"gpt-4\"\n",
    "\n",
    "\n",
    "class OpenAIClient:\n",
    "\n",
    "    _instance: OpenAIClient | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def instance(cls) -> OpenAIClient:\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self) -> None:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdc412",
   "metadata": {},
   "source": [
    "        # Ensure singleton creation only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb43360",
   "metadata": {},
   "outputs": [],
   "source": [
    "        if OpenAIClient._instance is not None:\n",
    "            raise RuntimeError(\"Use OpenAIClient.instance() instead of constructor\")\n",
    "\n",
    "        api_key = config.openai_api_key\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd873cd0",
   "metadata": {},
   "source": [
    "        # Use new OpenAI 1.x client (sync).  Async version via openai.AsyncOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ded8ff",
   "metadata": {},
   "source": [
    "Check if messages contain image content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c480d72",
   "metadata": {},
   "source": [
    "Get the appropriate model based on message content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3625de",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self._client = openai.AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "        self.model = config.openai_model\n",
    "        self.temperature = float(config.openai_temperature)\n",
    "        self.max_tokens = int(config.openai_max_tokens)\n",
    "\n",
    "    def _has_image_content(self, messages: list[dict[str, Any]]) -> bool:\n",
    "        for message in messages:\n",
    "            content = message.get('content', [])\n",
    "            if isinstance(content, list):\n",
    "                for item in content:\n",
    "                    if isinstance(item, dict) and item.get('type') == 'image_url':\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def _get_appropriate_model(self, messages: list[dict[str, Any]]) -> str:\n",
    "        if self._has_image_content(messages):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e0aca",
   "metadata": {},
   "source": [
    "            # Use vision-capable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if self.model in VISION_MODELS:\n",
    "                return self.model\n",
    "            else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d7471",
   "metadata": {},
   "source": [
    "                # Use a vision-capable model as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                vision_model = \"gpt-4o\"  # Most recent vision model\n",
    "                log.info(f\"Switching to vision-capable model: {vision_model}\")\n",
    "                return vision_model\n",
    "        else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bf458",
   "metadata": {},
   "source": [
    "            # Use configured model for text-only requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b67bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "            return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68276937",
   "metadata": {},
   "source": [
    "    # ---------------------------------------------------------------------\n",
    "    # Public API\n",
    "    # ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6603cbc",
   "metadata": {},
   "source": [
    "Send chat completion request and return assistant reply.\n",
    "\n",
    "        Args:\n",
    "            prompt: Convenience user prompt string. Ignored if ``messages`` is provided.\n",
    "            system_prompt: System prompt string (used if ``messages`` is ``None``).\n",
    "            messages: Full message list to pass through; takes precedence over *prompt*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858da0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    async def chat(\n",
    "        self,\n",
    "        *,\n",
    "        prompt: str | None = None,\n",
    "        system_prompt: str | None = None,\n",
    "        messages: list[dict[str, str]] | None = None,\n",
    "    ) -> str:\n",
    "        if messages is None:\n",
    "            if prompt is None:\n",
    "                raise ValueError(\"Either `messages` or `prompt` must be provided\")\n",
    "\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad020b",
   "metadata": {},
   "source": [
    "        # Otherwise: caller supplied full chat history via *messages*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        backoff = _BASE_BACKOFF\n",
    "        for attempt in range(_MAX_RETRIES):\n",
    "            try:\n",
    "                response = await self._client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=messages,  # type: ignore[arg-type]\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                )\n",
    "                content = response.choices[0].message.content  # type: ignore[attr-defined]\n",
    "                if content is None:\n",
    "                    raise RuntimeError(\"OpenAI returned empty content\")\n",
    "                return content\n",
    "            except (openai.APIError, openai.RateLimitError) as exc:\n",
    "                if attempt == _MAX_RETRIES - 1:\n",
    "                    log.error(f\"OpenAI request failed after {attempt+1} attempts: {exc}\")\n",
    "                    raise\n",
    "                sleep_time = backoff * (2 ** attempt) + random.uniform(0, 0.5)  # noqa: S311\n",
    "                log.warning(f\"OpenAI error {exc}. Retrying in {sleep_time:.1f}s…\")\n",
    "                await asyncio.sleep(sleep_time)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f777f",
   "metadata": {},
   "source": [
    "        # Should not reach here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c14cb",
   "metadata": {},
   "source": [
    "Send chat completion request with support for text and image messages.\n",
    "\n",
    "        Args:\n",
    "            messages: List of message dictionaries that can include text and image content.\n",
    "\n",
    "        Returns:\n",
    "            Parsed response as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        raise RuntimeError(\"OpenAI chat completion failed after retries\")\n",
    "\n",
    "    async def get_completion(\n",
    "        self,\n",
    "        messages: list[dict[str, Any]],\n",
    "    ) -> dict[str, Any]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0b7cc",
   "metadata": {},
   "source": [
    "        # Determine appropriate model based on content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        model = self._get_appropriate_model(messages)\n",
    "        \n",
    "        backoff = _BASE_BACKOFF\n",
    "        for attempt in range(_MAX_RETRIES):\n",
    "            try:\n",
    "                response = await self._client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,  # type: ignore[arg-type]\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                )\n",
    "                content = response.choices[0].message.content  # type: ignore[attr-defined]\n",
    "                if content is None:\n",
    "                    raise RuntimeError(\"OpenAI returned empty content\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60e009",
   "metadata": {},
   "source": [
    "                # Try to parse as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a56530",
   "metadata": {},
   "outputs": [],
   "source": [
    "                try:\n",
    "                    import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c1b68",
   "metadata": {},
   "source": [
    "                    # Find JSON in the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a475c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    start_idx = content.find('{')\n",
    "                    end_idx = content.rfind('}') + 1\n",
    "                    if start_idx != -1 and end_idx > start_idx:\n",
    "                        json_str = content[start_idx:end_idx]\n",
    "                        return json.loads(json_str)\n",
    "                    else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37f5b7",
   "metadata": {},
   "source": [
    "                        # If no JSON found, return as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        return {\"analysis\": {\"reasoning\": content}}\n",
    "                except json.JSONDecodeError:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c3778",
   "metadata": {},
   "source": [
    "                    # If JSON parsing fails, return as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    return {\"analysis\": {\"reasoning\": content}}\n",
    "                    \n",
    "            except (openai.APIError, openai.RateLimitError) as exc:\n",
    "                if attempt == _MAX_RETRIES - 1:\n",
    "                    log.error(f\"OpenAI request failed after {attempt+1} attempts: {exc}\")\n",
    "                    raise\n",
    "                sleep_time = backoff * (2 ** attempt) + random.uniform(0, 0.5)  # noqa: S311\n",
    "                log.warning(f\"OpenAI error {exc}. Retrying in {sleep_time:.1f}s…\")\n",
    "                await asyncio.sleep(sleep_time)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83662428",
   "metadata": {},
   "source": [
    "        # Should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        raise RuntimeError(\"OpenAI chat completion failed after retries\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebbff8",
   "metadata": {},
   "source": [
    "# Convenience getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_openai_client = OpenAIClient.instance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
